{
  "comments": [
    {
      "key": {
        "uuid": "d1bc8c3f_2a23dee1",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "I strongly disagree with this change - having unpriviliged user was needed to run rke - so now it runs under root, or how?",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "7459e248_934f730c",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T11:21:13Z",
      "side": 1,
      "message": "If I understood correctly then you DO use rke for ssh - it is in the cluster.yml (user: rke) - the reason for this artificial user was that on the CentOS/RHEL 7 ssh local-forwarding does not work for root - so rke is using unprivileged user for that. You are running rke up as root (or another ansible user) but the rke is internally using rke user for ssh and distribution its artefacts - so what was actually achieved with this change? Am I wrong? Please, correct me.",
      "parentUuid": "d1bc8c3f_2a23dee1",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "58a09d0c_f57236d2",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "Yes, the intention is for rke to run as root, but to ssh to stuff as rke. I left the rke user on infra mainly because we may want/will want? to move for example control plane to infra, so it will just work without having to do some unusual things.\nAs for why you would run rke as root: I believe we are not testing offline installer running as different users with become: yes, but we really shouldn\u0027t block this usage. In that case using become like in previous patchsets would introduce another hidden requirement that the user that ansible connects as must have sudo rights for root and also for rke, without a password. Another way would be to use sudo from root context, but that seems a bad idea/bad practice. So the most clean way to handle this was to just run rke as root, and create the rke user mostly just to distribute ssh keys and in case some components would be moved to infra. Also it would be consistent with all other roles that run everything as root (helm/etc).",
      "parentUuid": "7459e248_934f730c",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2503eb70_cf8cebd3",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-06T11:24:24Z",
      "side": 1,
      "message": "Ok, that \"become\" point seems valid",
      "parentUuid": "58a09d0c_f57236d2",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "cd7993c3_3b08fc22",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 9,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "on install server does not have to be any docker - this does not fix anything, rke user is for running rke and to deploy k8s on different hosts...",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4c026acb_e4d82cac",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 9,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T11:21:13Z",
      "side": 1,
      "message": "my bad, this is infra server, nevertheless it is still not needed - rke playbook does not use rke user to modify docker on infra, although it does not hurt and infra should have always docker, so I take this back",
      "parentUuid": "cd7993c3_3b08fc22",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "176a06e9_e8f20fc1",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 20,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "the rke user has purpose to contain rke stuff inside...now it is polluted in user which ansible uses..why?",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "fb30bd84_e263b1a0",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 20,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "mostly consistency and, see comment for line #5 for why it is better for rke to run as root.",
      "parentUuid": "176a06e9_e8f20fc1",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5ebcd03d_9ef72d89",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 26,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "yes, I am aware of that - it was the simplest solution and \"grow without bounds\" is kind of overstatement - how many times this playbook will be run? Once?",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0bda8085_661d75c9",
        "filename": "ansible/roles/rke/tasks/rke_config.yml",
        "patchSetId": 3
      },
      "lineNbr": 26,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "let\u0027s not assume users will not do stupid things if fixing that is not difficult. :) generally you\u0027re right, but we should still try to make stuff idempotent and this wouldn\u0027t be quite idempotent if it didn\u0027t handle that case.",
      "parentUuid": "5ebcd03d_9ef72d89",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3f7c58a2_65e47104",
        "filename": "ansible/roles/rke/tasks/rke_deploy.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "we now running this from what user exactly?",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6ad7c70d_7b96a31f",
        "filename": "ansible/roles/rke/tasks/rke_deploy.yml",
        "patchSetId": 3
      },
      "lineNbr": 5,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "from root (either over become or normally/directly).",
      "parentUuid": "3f7c58a2_65e47104",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "aa5b9713_a260028e",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 4,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "why you removed the default gid? what is default now - it is platform specific, it could be users now or whatnot",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1d256cb5_e0dab098",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 4,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "well i believe for all newer systems it is not users but the group named after the user, but the reason was:\n1- do we actually care about this group? we don\u0027t use it for anything.\n2-I don\u0027t remember what was the exact reason but I believe something has failed because of this...",
      "parentUuid": "aa5b9713_a260028e",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "209a8bfe_68b016b4",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "this: why? do we plan more infra servers or what is the reason to complicate it and hardwire one with index 0",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "88fae429_d592f29c",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 3957
      },
      "writtenOn": "2019-05-02T08:59:52Z",
      "side": 1,
      "message": "we never have fixed the amount of infra servers to one.",
      "parentUuid": "209a8bfe_68b016b4",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "87b7052e_f8ed0d2d",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T11:21:13Z",
      "side": 1,
      "message": "I dont have anything against mutiple of infra - but how would that work? And why we elevate the one with index 0 - if we do support configuration of multiple infra nodes, then fine - but this obviously is not that.",
      "parentUuid": "88fae429_d592f29c",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "06f6d4a5_f81d5e17",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 3957
      },
      "writtenOn": "2019-05-02T11:45:36Z",
      "side": 1,
      "message": "that should be changed in all places as there are many places in offline-installer where we index with first server in the group.",
      "parentUuid": "87b7052e_f8ed0d2d",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1214d974_0ce5a6f8",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "the reason is that you should not rely on name of hosts in inventory, only on names of groups. As in, using hostname directly would not work for cicdansible generated inventory without changing cicdansible.",
      "parentUuid": "06f6d4a5_f81d5e17",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a8d3ea27_5573365f",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-06T11:24:24Z",
      "side": 1,
      "message": "This maybe my insufficient knowledge of ansible but when I look at infrastructer-server in hosts.yaml then it does not look like name of the host but as a dictionary and host is defined below...",
      "parentUuid": "1214d974_0ce5a6f8",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "23c82aa3_077f4ff7",
        "filename": "ansible/roles/rke/tasks/rke_node.yml",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T12:28:10Z",
      "side": 1,
      "message": "it is the ansible visible name of the host (the dict key). but I believe normal ansible practice is to lookup hosts from groups. that previous version would force me to name this key \"infrastructure-server\". however in any case, infrastructure group does exist and there is one host there.",
      "parentUuid": "a8d3ea27_5573365f",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "47a66a7c_50f61f6b",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "wrong, since the beginning when I proposed to add cluster_ip, it was due to multiple network interfaces, asnible is connecting to some network interface and the cluster runs on another and both of them can be mutually unreachable...",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3f14af51_0f996b1e",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "Well, they may be mutually unreachable, but I am not sure if you are going to set up ssh not to run on the internal cluster network. I assumed it can run on internal cluster network too.\nThe reason for this change was molecule, because it doesn\u0027t use the ssh connection driver, it uses the docker connection driver, where ansible_host seems to be container name. So, using ansible_host had bad side effects, because it was invalid in molecule context.",
      "parentUuid": "47a66a7c_50f61f6b",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a83a63c_8e139201",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 3957
      },
      "writtenOn": "2019-05-06T10:49:17Z",
      "side": 1,
      "message": "I\u0027m glad someone else also fit this problem. I.e. in molecule context ansible_host is not used/filled as it\u0027s our own addition to inventory not there by default.",
      "parentUuid": "3f14af51_0f996b1e",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0dfa3276_9d75ff3b",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T10:56:44Z",
      "side": 1,
      "message": "well actually ansible_host is used and it is the worst. if you set it then it is treated as container name.",
      "parentUuid": "1a83a63c_8e139201",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c354146f_5dcbf558",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-06T11:24:24Z",
      "side": 1,
      "message": "I dont follow at all. If this is needed for that molecule to work then that is sad, but ok then. Otherwise - this address is address which rke uses to connect over ssh...from install server/ansible and that can be special management network with restrictions. But I already explained this so many times before, that I dont want to repeat it again.",
      "parentUuid": "0dfa3276_9d75ff3b",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b05a93aa_122a3750",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 13,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T12:28:10Z",
      "side": 1,
      "message": "sure. but I do not see any alternative, using ansible_host for ssh in case of molecule is asking for trouble, because the docker connection plugin will interpret it as a docker container name. Using cluster_ip would require ssh to be exposed also on the internal cluster network, but at least it works now. Not sure if we have any viable alternative. You rather cannot use docker container names as dns names to ssh to the containers, so...",
      "parentUuid": "c354146f_5dcbf558",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "82242f58_fec0e841",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 123,
      "author": {
        "id": 4886
      },
      "writtenOn": "2019-05-02T08:45:46Z",
      "side": 1,
      "message": "why? because onap is broken? rbac is standard k8s policy framework and it was said that we want to make onap more securish",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3d7ecbcc_fc59d8de",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 123,
      "author": {
        "id": 3957
      },
      "writtenOn": "2019-05-02T08:59:52Z",
      "side": 1,
      "message": "onap does not take any role for underlay Kubernetes cluster. It\u0027s totally upto onap users (vendors and operators) how thay want to deploy Kubernetes.\n\nI guess this is default value that can be overriden in any deployment?",
      "parentUuid": "82242f58_fec0e841",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5fedddb8_c0e92856",
        "filename": "ansible/roles/rke/templates/cluster.yml.j2",
        "patchSetId": 3
      },
      "lineNbr": 123,
      "author": {
        "id": 5019
      },
      "writtenOn": "2019-05-06T09:11:03Z",
      "side": 1,
      "message": "1. I assumed it is better not to introduce additional problems because of rbac, and\n2. using rbac would require modifications to helm init usage, you would need a service account for helm, I once tried that when I was playing with kubeadm long ago (amsterdam times) and it needed permissions that it did not have by default. So using none here was just easier than doing more extensive changes (at least for this initial release of rke role).",
      "parentUuid": "3d7ecbcc_fc59d8de",
      "revId": "86b68e4da742a4abeb711846ada30424b1914b98",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    }
  ]
}